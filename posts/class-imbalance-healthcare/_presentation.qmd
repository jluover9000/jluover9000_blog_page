---
title: "Dealing With Imbalanced Healthcare Data"
subtitle: "Jackson Lu"
listing: false
resources:
  - img/*.png
  - _presentation_files
format:
  revealjs:
    theme: dark
    transition: slide
    slide-number: true
---

## What is Class Imbalance?

![](img/piechart.png){width=70%}

::: {.notes}
Today I'm hope to convince you to use Balanced tree based models for imbalanced healthcare data and not immediately use techniques like oversampling or undersampling in your training data.
:::
::: {.notes}
Class imbalance is everywhere in healthcare data. In a typical medical imaging dataset, we might see 95% healthy scans and only 5% showing disease. This mirrors reality where most people are healthy.
But here's the catch: when we train a model on this imbalanced data, it becomes really good at identifying healthy cases and terrible at detecting the 5% that actually matters.
In healthcare, the minority class is often the most critical class.
:::

---

## Two Approaches

:::: {.columns}
::: {.column width="50%"}
### Data-Level Approach
- Oversampling (↑ minority)
- Undersampling (↓ majority)
:::

::: {.column width="50%"}
### Algorithm-Level Approach
- Balanced Random Forest
- Balanced Bagging
:::
::::

::: {.notes}
There are two main approaches. 
First, data-level: where you manipulate your training data by synthetically boosting the minority class or randomly remove majority class to create a balanced dataset then feed that to the model.
Think of it like studying from a manipulated textbook that doesn't reflect reality.

Second, algorith-level: you look for models that pay extra attention to minority class without changing training data. Balanced Random Forest and Balanced Bagging are two models that does exactly this.
They have built in parameter that lets you set the weights. 
:::

---

## Which Works Better?

![](img/avgranking.png){width=80%}

<p style="font-size: 0.5em; font-style: italic;">Source: Martínez-Velasco et al., IEEE LATIN AMERICA TRANSACTIONS (2024)</p>

::: {.notes}
So which approach actually works better? 
According to this chart from a publication i found on IEEE. 
It ranks different methods based on balanced accuracy which is the average score of recall and specificity.
which is what matters in medical diagnosis.
Look at the top: Balanced Bagging and Balanced Random Forest dominate the rankings. Now look at the bottom: non-tree models like SVM and KNN, even when combined with oversampling and undersampling.

Why? The tree-based models handle imbalance during the learning process itself—each tree is built with balanced bootstrapped samples, so they naturally weight rare cases properly. In contrast, SVM and KNN rely on geometric distance calculations that get overwhelmed by the majority class. Oversampling helps, but it's a band-aid, not a structural fix."
:::

---

## Key Takeaway

::: {.callout-important}
## Bottom Line
Start here: Balanced Random Forest
:::

::: {.fragment}
✓ More effective  
✓ Easier to implement  
✓ Designed for this problem
:::

::: {.notes}
So here's the bottom line: if you're working with imbalanced healthcare data, skip oversampling and undersampling, go straight to tree-based models like Balanced Random Forest. They're more effective, easier to implement, and specifically designed for this problem. Thank you!
:::
